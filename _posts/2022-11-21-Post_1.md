---
layout: post
title: Sobre el Sesgo en Inteligencia Artifical y sus Efectos en Sistemas de Recomendación 
---

## Sesgo e Inteligencia Artificial
En la actualidad, cada día nos sorprendemos con nuevos avances en tecnología, especialmente en el área de la inteligencia Artificial. En efecto, a menudo escuchamos sobre vehículos autónomos, nuevos algoritmos y aplicaciones de procesamiento de lenguaje natural, generación de imágenes a partir de lenguaje natural, algoritmos que le ganan a campeones mundiales en juegos de mesa (como Go), redes generativas que pueden crear videos y escenas completas, entre muchos otros.
Sin embargo, hay algunos efectos problemáticos a partir de la aplicación de estas tecnologías, relacionados con el sesgo en la toma de decisiones o predicciones, que merecen la pena su discusión y la formulación de algunas posibles soluciones. 

### El ejemplo de COMPAS
Un ejemplo muy conocido es el del software llamado Correctional Offender Management Profiling for Alternative Sanctions (o COMPAS, por su sigla). Este producto construido por Tim Brennan (profesor de Estadística en la Universidad de Colorado) y Dave Wells (para el momento de la creación del software, administrador de un programa correccional en Traverse City, Michigan). Ambos autores, declarados fans de lo que denominan “taxonomía cuantitativa”: la medición de rasgos de personalidad como la inteligencia, la extroversión y la introversión. Juntos, decidieron construir un *score* de evaluación de riesgos para la industria penitenciaria. De esta forma, una vez liberado el sistema, muchas jurisdicciones comenzaron a utilizarlo, inicialmente en programas piloto. 

Por ejemplo, el Estado de New York comenzó a utilizarlo en 2001, pero no fue sino hasta 2012 que se publicó una detallada evaluación estadística, donde se indicaba que de 16,000 personas en libertad condicional, la herramienta tenía una precisión del 71%, pero sin haber evaluado diferencias raciales. El Estado de New York entregó sucintas explicaciones para esta omisión.
En 2009 Brennan y dos colegas publicaron un [estudio de validación](http://www.northpointeinc.com/files/publications/Criminal-Justice-Behavior-COMPAS.pdf) que encontró que COMPAS tenía una tasa de precisión del 68% en una muestra de 2328 personas. Su estudio también encontró que la puntuación era ligeramente menos *predictiva* para los hombres negros que para los hombres blancos, pero no profundizó en este último punto, mencionando que eliminar variables relacionadas con la raza disminuía notablemente la precisión del sistema.
Cabe mencionar que los jueces no debían asignar penas mayores en sus sentencias a los *scores* más altos. Más bien, debían usar ese puntaje para tomar decisiones en cuanto a programas de rehabilitación o libertad condicional. Sin embargo, jueces comenzaron a utilizar abiertamente en sus sentencias las referencias a altos valores en el *score* para justificar penas mayores por el alto riesgo de reincidencia.

Generalmente, COMPAS predice correctamente la reincidencia el 61% de las veces. Pero como se observa en la siguiente tabla, los afroamericanos tienen casi el doble de probabilidades que los blancos de ser etiquetados como de mayor riesgo, pero en realidad no reinciden. Se comete el error opuesto entre los blancos: son mucho más propensos que los afroamericanos a ser etiquetados como de menor riesgo pero continúan cometiendo otros delitos.

![image](https://user-images.githubusercontent.com/84675120/205516791-c4954d0d-f954-4b3c-8053-e5d79a25fbf3.png)

De acuerdo a este análisis y a los casos específicos presentados [en el artículo por ProPublica](https://www.propublica.org/article/machine-bias-risk-assessments-in-criminal-sentencing), existe un riesgo mayor de sesgo racial en el uso de COMPAS.

### Gender Shades
Joy Buolamwini, graduada del MIT Media Lab, casualmente utilizó un software de reconocimiento facial. Para su sorpresa, el software no fue capaz de identificar su género (sexo biológico). A partir de esta experiencia, Joy decidió realizar una investigación, fundando posteriormente la organización Gender Shades, para poder determinar el grado de sesgo de género en los sistemas de reconocimiento facial.
En un experimento, se seleccionaron 1270 imágenes para crear una línea base para una prueba de rendimiento de clasificación de género. Los sujetos fueron seleccionados de 3 países africanos y 3 países europeos. Luego, los sujetos se agruparon por género, tipo de piel y la intersección de género y tipo de piel. El género se dividió en categorías femeninas y masculinas, ya que los productos de software evaluados proporcionan etiquetas binarias de sexo para la función de clasificación de género. 
En el análisis de resultados, todas los productos obtienen mejores resultados con los hombres que con las mujeres, con una diferencia del 8,1% al 20,6% en las tasas de error. Cuando se analizaron los resultados por sub-grupos interseccionales (hombres de pieles más oscuras, mujeres de pieles más oscuras, hombres de pieles más claras, mujeres de pieles más claras), se observa que todas los productos se desempeñan peor en las mujeres de pieles más oscuras.

![image](https://user-images.githubusercontent.com/84675120/205518132-6a586547-cef7-4415-aad4-4f980d429b6e.png)

Estos resultados revela la necesidad de hacerse cargo del problema de sesgo que potencialmente conlleva discriminación, desarrollando tecnologías, metodologías y principios que resuelvan estas dificultades.

## ¿Y esto puede afectar a los Sistemas de Recomendación?

La respuesta a priori es si.

Comencemos por caracterizar a los Sistemas de Recomendación, los cuales ayudan a las personas a filtrar el ruido, identificando ítems relevantes en enormes espacios de información. Éstos son usualmente optimizados en *accuracy* y métricas de *ranking*, no en justicia (*fairness*). 

### YouTube

Podemos tomar el ejemplo del algoritmo del [algoritmo de YouTube de 2016](https://dl.acm.org/doi/10.1145/2959100.2959190), compuesto de dos redes, una para generación de candidatos y la otra para el ranking, donde las tareas eran predecir el siguiente video a ver y el tiempo a invertir viéndolo. Sin embargo, no eran parte de las tareas determinar o prevenir si el contenido era de buena o mala calidad (como fake news, violencia, entre otros). 
Fue así como a partir del interés personal de personas como Guillaume Chaslot (ex ingeniero de YouTube), que realizó experimentos que mostraron [resultados preocupantes en las recomendaciones](https://www.theguardian.com/technology/2018/feb/02/youtube-algorithm-election-clinton-trump-guillaume-chaslot), se realizaron otros estudios, como [Many Turn to YouTube for Children’s Content, News, How-To Lessons](https://www.pewresearch.org/internet/2018/11/07/many-turn-to-youtube-for-childrens-content-news-how-to-lessons/), donde se evaluó si personas de diferentes grupos etarios consumía o no las recomendaciones, identificando que de manera regular o en ocasiones, al menos el 75% seguía las recomendaciones, por lo tanto era un tema relevante. Además de esto, se encontró que YouTube progresivamente guía hacia videos más largos y más populares en sus recomendaciones, lo cual puede tener como consecuencia efectos en la diversidad de contenido que se observa como grupo. En 2019, YouTube lanzó un nuevo recomendador, donde se agregan elementos no utilizadas antes, como likes, señales de engagement y señales de satisfacción.
Recientemente, se publicó un artículo en The New York Times, que revela que en un estudio del navegador de Mozilla, a pesar que los usuarios pinchaban el. botón de dislike para algún video, de todas formas volvía aparecer el mismo video o similares. 

## ¿Qué se puede hacer?

### Aspectos Regulatorios

El 25 de mayo de 2018, la ley de  general de protección de datos de la Unión Europea se hizo exigible, aunque ya existía previamente. Esta regulación afecta no solo a organizaciones localizadas en la Unión Europea, sino a todas aquellas que le ofrecen sus bienes y servicios o monitorean comportamiento de *sujetos de dato* de la Unión Europea, todo esto independiente de la localización de la empresa. Uno de los artículos, específicamente el 71 de la regulación, implica una suerte de derecho a la explicación, lo cual presenta un desafío mayor a tecnologías como redes neuronales profundas, donde pueden haber cientos de millones de variables.

### ¿Qué se está haciendo para resolver el problema de la explicabilidad?

Se han creado diversas iniciativas con la idea de abordar estos problemas.

 - **La conferencia FAccT (ex FAT)**. Esta conferencia agrupa no solo a profesionales de tecnología, sino otras disciplinas, como abogados, sociólogos, entre otros.
 - **El grupo FATML**. Creado con la intención de generar una serie de principios para la generación de algoritmos 'responsables' (Accountables, que puedan dar explicaciones) y acerca del impacto social de estos algoritmos.

## ¿Cómo medir, estudiar y prevenir el sesgo en Sistemas de Recomendación?

### Algunas definiciones (FAT)

 - **Fairness**. En oposición a 'sesgo'. La propiedad de ser justo o equitativo. De acuerdo a Friedman y Nissembaum (1994), un sistema es sesgado si 'sistemáticamente discrimina en contra de ciertos individuos o grupos de individuos en favor de otros.'.
 - **Accountability**. Poder dar cuenta o reportar, explicar o justificar algo. Es decir, IA explicable (**XAI**).
 - **(Algorithmic) Transparency**. Los principios y factors que influencian las decisiones de un algoritmos deberían ser transparentes para las personas que la usan, regulan y son afectadas por éstas.

### Explainable Artificial Intelligence (XAI)

Acuñado por David Gunning, es especialmente necesario para aplicaciones en sectores críticos, como Trasporte, Seguridad, Medicina, Finanzas, Legal, Militar, pudiendo responder preguntas como ¿Por qué se hizo eso?, ¿Por qué no algo diferente?, ¿Cuándo se es exitoso o se logra?,  ¿Cuándo de falla?, ¿Cuándo puedo confiar?, ¿Cómo corrijo un error?.

A diferencia de la primera generación de RecSys, donde la explicabilidad estaba implícita en los modelos más sencillos utilizados (como K-NN, por ejemplo), la segunda generación de RecSys, que incluía por ejemplo Factorización Matricial, presentaba un desafío nuevo. En 2014, Zhang propuso en su paper un enfoque basado en factores latentes en factorización matricial, usando comentarios de los usuarios, identificando lo que llama 'aspectos' para lograr la explicación.

En la tercera generación de RecSys, al utilizar Deep Learning, la explicabilidad se convierte en una tarea no trivial. Entre las alternativas chequeadas, se ha propuesto usar la 'atención neuronal', así como otras propuestas relacionadas también con atención, entre otros elementos.

### Visualización Interactiva en Sistemas de Recomendación XAI

Algunos de los variados sistemas que utilizan visualización interactiva:

 - **PeerChooser** (2008). A través de visualización de usuarios similares, entender la recomendación de películas.
 - **SmallWorld** (2010). Implementado como aplicación de Facebook, similar a PeerChooser, utilizando API de Facebook.
 - **Each to his own** (2010). Basado en reglas para proponer acciones para reducir consumo doméstico de energía.
 - **TasteWeights** (2012). Sistema híbrido, para recomendación de música, donde se muestra la información de contexto relacionada con las recomendaciones, así como cambiar la importancia relativa de elementos del contexto.
 - **TalkExplorer** (2013). Recomendador de papers. Al seleccionar agentes y/o persona generaba clusters de elementos sugeridos.
 - **MoodPlay** (2019). Utilizando la emoción que los artistas proyectan para recomendar 
 
 #### Desafíos abiertos
 - Aprovechamiento de los avances en procesamiento de lenguaje natural y modelos generativos
 - Aprovechar más la visualización para recomendar

#### Referencias

 - [Machine bias risk assessment in criminal sentencing](https://www.propublica.org/article/machine-bias-risk-assessments-in-criminal-sentencing)
 - [Proyecto Gender Shades](https://www.media.mit.edu/projects/gender-shades/overview/)
 - [Study finds gender and skin-type bias in commercial artificial-intelligence systems](https://news.mit.edu/2018/study-finds-gender-skin-type-bias-artificial-intelligence-systems-0212)
 - [Joy Buolamwini: How Does Facial Recognition Software See Skin Color?](https://www.npr.org/2018/01/26/580619086/joy-buolamwini-how-does-facial-recognition-software-see-skin-color)
 - [Gender Shades](http://gendershades.org/overview.html)
 - [Deep Neural Networks for YouTube Recommendations](https://dl.acm.org/doi/10.1145/2959100.2959190)
 - [How an ex-YouTube insider investigated its secret algorithm](https://www.theguardian.com/technology/2018/feb/02/youtube-algorithm-election-clinton-trump-guillaume-chaslot)

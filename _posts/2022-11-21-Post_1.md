---
layout: post
title: Sobre el Sesgo en Inteligencia Artifical y sus Efectos en Sistemas de Recomendación 
---

## Sesgo e Inteligencia Artificial
En la actualidad, cada día nos sorprendemos con nuevos avances en tecnología, especialmente en el área de la inteligencia Artificial. En efecto, a menudo escuchamos sobre vehículos autónomos, nuevos algoritmos y aplicaciones de procesamiento de lenguaje natural, generación de imágenes a partir de lenguaje natural, algoritmos que le ganan a campeones mundiales en juegos de mesa (como Go), redes generativas que pueden crear videos y escenas completas, entre muchos otros.
Sin embargo, hay algunos efectos problemáticos a partir de la aplicación de estas tecnologías, relacionados con el sesgo en la toma de decisiones o predicciones, que merecen la pena su discusión y la formulación de algunas posibles soluciones. 

### El ejemplo de COMPAS
Un ejemplo muy conocido es el del software llamado Correctional Offender Management Profiling for Alternative Sanctions (o COMPAS, por su sigla). Este producto construido por Tim Brennan (profesor de Estadística en la Universidad de Colorado) y Dave Wells (para el momento de la creación del software, administrador de un programa correccional en Traverse City, Michigan). Ambos autores, declarados fans de lo que denominan “taxonomía cuantitativa”: la medición de rasgos de personalidad como la inteligencia, la extroversión y la introversión. Juntos, decidieron construir un *score* de evaluación de riesgos para la industria penitenciaria. De esta forma, una vez liberado el sistema, muchas jurisdicciones comenzaron a utilizarlo, inicialmente en programas piloto. 

Por ejemplo, el Estado de New York comenzó a utilizarlo en 2001, pero no fue sino hasta 2012 que se publicó una detallada evaluación estadística, donde se indicaba que de 16,000 personas en libertad condicional, la herramienta tenía una precisión del 71%, pero sin haber evaluado diferencias raciales. El Estado de New York entregó sucintas explicaciones para esta omisión.
En 2009 Brennan y dos colegas publicaron un [estudio de validación](http://www.northpointeinc.com/files/publications/Criminal-Justice-Behavior-COMPAS.pdf) que encontró que COMPAS tenía una tasa de precisión del 68 por ciento en una muestra de 2328 personas. Su estudio también encontró que la puntuación era ligeramente menos *predictiva* para los hombres negros que para los hombres blancos, pero no profundizó en este último punto, mencionando que eliminar variables relacionadas con la raza disminuía notablemente la precisión del sistema.
Cabe mencionar que los jueces no debían asignar penas mayores en sus sentencias a los *scores* más altos. Más bien, debían usar ese puntaje para tomar decisiones en cuanto a programas de rehabilitación o libertad condicional. Sin embargo, jueces comenzaron a utilizar abiertamente en sus sentencias las referencias a altos valores en el *score* para justificar penas mayores por el alto riesgo de reincidencia.

Generalmente, COMPAS predice correctamente la reincidencia el 61% de las veces. Pero como se observa en la siguiente tabla, los afroamericanos tienen casi el doble de probabilidades que los blancos de ser etiquetados como de mayor riesgo, pero en realidad no reinciden. Se comete el error opuesto entre los blancos: son mucho más propensos que los afroamericanos a ser etiquetados como de menor riesgo pero continúan cometiendo otros delitos.

![image](https://user-images.githubusercontent.com/84675120/205516791-c4954d0d-f954-4b3c-8053-e5d79a25fbf3.png)

De acuerdo a este análisis y a los casos específicos presentados [en el artículo por ProPublica](https://www.propublica.org/article/machine-bias-risk-assessments-in-criminal-sentencing), existe un riesgo mayor de sesgo racial en el uso de COMPAS.

### Gender Shades
Joy Buolamwini, graduada del MIT Media Lab, casualmente utilizó un software de reconocimiento facial. Para su sorpresa, el software no fue capaz de identificar su género (sexo biológico). A partir de esta experiencia, Joy decidió realizar una investigación, fundando posteriormente la organización Gender Shades, para poder determinar el grado de sesgo de género en los sistemas de reconocimiento facial.
En un experimento, se seleccionaron 1270 imágenes para crear una línea base para una prueba de rendimiento de clasificación de género. Los sujetos fueron seleccionados de 3 países africanos y 3 países europeos. Luego, los sujetos se agruparon por género, tipo de piel y la intersección de género y tipo de piel. El género se dividió en categorías femeninas y masculinas, ya que los productos de software evaluados proporcionan etiquetas binarias de sexo para la función de clasificación de género. 
En el análisis de resultados, todas los productos obtienen mejores resultados con los hombres que con las mujeres, con una diferencia del 8,1% al 20,6% en las tasas de error. Cuando se analizaron los resultados por sub-grupos interseccionales (hombres de pieles más oscuras, mujeres de pieles más oscuras, hombres de pieles más claras, mujeres de pieles más claras), se observa que todas los productos se desempeñan peor en las mujeres de pieles más oscuras.

![image](https://user-images.githubusercontent.com/84675120/205518132-6a586547-cef7-4415-aad4-4f980d429b6e.png)

Estos resultados revela la necesidad de hacerse cargo del problema de sesgo que potencialmente conlleva discriminación, desarrollando tecnologías, metodologías y principios que resuelvan estas dificultades.



#### Referencias

 - [Machine bias risk assessment in criminal sentencing](https://www.propublica.org/article/machine-bias-risk-assessments-in-criminal-sentencing)
 - [Proyecto Gender Shades](https://www.media.mit.edu/projects/gender-shades/overview/)
 - [Study finds gender and skin-type bias in commercial artificial-intelligence systems](https://news.mit.edu/2018/study-finds-gender-skin-type-bias-artificial-intelligence-systems-0212)
 - [Joy Buolamwini: How Does Facial Recognition Software See Skin Color?](https://www.npr.org/2018/01/26/580619086/joy-buolamwini-how-does-facial-recognition-software-see-skin-color)
 - [Gender Shades](http://gendershades.org/overview.html)

---
layout: post
title: Sobre el Sesgo en Inteligencia Artifical y sus Efectos en Sistemas de Recomendación 
---

## Sesgo e Inteligencia Artificial
En la actualidad, cada día nos sorprendemos con nuevos avances en tecnología, especialmente en el área de la inteligencia Artificial. En efecto, a menudo escuchamos sobre vehículos autónomos, nuevos algoritmos y aplicaciones de procesamiento de lenguaje natural, generación de imágenes a partir de lenguaje natural, algoritmos que le ganan a campeones mundiales en juegos de mesa (como Go), redes generativas que pueden crear videos y escenas completas, entre muchos otros.
Sin embargo, hay algunos efectos problemáticos a partir de la aplicación de estas tecnologías, relacionados con el sesgo en la toma de decisiones o predicciones, que merecen la pena su discusión y la formulación de algunas posibles soluciones. 

### El ejemplo de COMPAS
Un ejemplo muy conocido es el del software llamado Correctional Offender Management Profiling for Alternative Sanctions (o COMPAS, por su sigla). Este producto construido por Tim Brennan (profesor de Estadística en la Universidad de Colorado) y Dave Wells (para el momento de la creación del software, administrador de un programa correccional en Traverse City, Michigan). Ambos autores, declarados fans de lo que denominan “taxonomía cuantitativa”: la medición de rasgos de personalidad como la inteligencia, la extroversión y la introversión. Juntos, decidieron construir un *score* de evaluación de riesgos para la industria penitenciaria. De esta forma, una vez liberado el sistema, muchas jurisdicciones comenzaron a utilizarlo, inicialmente en programas piloto. Por ejemplo, el Estado de New York comenzó a utilizarlo en 2001, pero no fue sino hasta 2012 que se publicó una detallada evaluación estadística, donde se indicaba que de 16,000 personas en libertad condicional, la herramienta tenía una precisión del 71%, pero sin haber evaluado diferencias raciales. El Estado de New York entregó sucintas explicaciones para esta omisión.
En 2009 Brennan y dos colegas publicaron un [estudio de validación](http://www.northpointeinc.com/files/publications/Criminal-Justice-Behavior-COMPAS.pdf) que encontró que COMPAS tenía una tasa de precisión del 68 por ciento en una muestra de 2328 personas. Su estudio también encontró que la puntuación era ligeramente menos *predictiva* para los hombres negros que para los hombres blancos, pero no profundizó en este último punto, mencionando que eliminar variables relacionadas con la raza disminuía notablemente la precisión del sistema.
Cabe mencionar que los jueces no debían asignar penas mayores en sus sentencias a los *scores* más altos. Más bien, debían usar ese puntaje para tomar decisiones en cuanto a programas de rehabilitación o libertad condicional. Sin embargo, jueces comenzaron a utilizar abiertamente en sus sentencias las referencias a altos valores en el *score* para justificar penas mayores por el alto riesgo de reincidencia.

Generalmente, COMPAS predice correctamente la reincidencia el 61% de las veces. Pero como se observa en la siguiente tabla, los afroamericanos tienen casi el doble de probabilidades que los blancos de ser etiquetados como de mayor riesgo, pero en realidad no reinciden. Se comete el error opuesto entre los blancos: son mucho más propensos que los negros a ser etiquetados como de menor riesgo pero continúan cometiendo otros delitos.

![image](https://user-images.githubusercontent.com/84675120/205516791-c4954d0d-f954-4b3c-8053-e5d79a25fbf3.png)

De acuerdo a este análisis y a los casos específicos presentados [en el artículo por ProPublica](https://www.propublica.org/article/machine-bias-risk-assessments-in-criminal-sentencing), existe un riesgo mayor de sesgo racial en el uso de COMPAS.

### Referencias

 - [Machine bias risk assessment in criminal sentencing](https://www.propublica.org/article/machine-bias-risk-assessments-in-criminal-sentencing)
 - 
 - 
